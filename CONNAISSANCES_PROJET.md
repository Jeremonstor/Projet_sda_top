# üìö Guide des Connaissances - Projet Accidents V√©lo IDF

**Projet Sciences des Donn√©es et Apprentissage 2025/2026**  
*Analyse et Pr√©diction des Accidents de V√©lo en √éle-de-France*

---

## üéØ Vue d'ensemble du projet

Ce projet fusionne **4 datasets** (accidents, am√©nagements cyclables, comptages v√©lo, population INSEE) pour analyser et pr√©dire les accidents de v√©lo en √éle-de-France via 3 approches ML :
1. **Classification** : Identifier les communes √† risque √©lev√©
2. **R√©gression** : Pr√©dire le nombre brut d'accidents
3. **R√©gression sur taux normalis√©s** : Pr√©dire des taux de risque (par km, par habitant)

---

## üìÇ PARTIE 1 : Recherche & Conception des Features
*David Chhoa & J√©r√©mie Masnou*

### 1.1 Recherche de donn√©es sur data.gouv.fr

#### Comp√©tences requises
- **Navigation efficace** sur les portails open data
- **√âvaluation de la qualit√©** des datasets :
  - Compl√©tude (% de valeurs manquantes)
  - Fra√Æcheur (date de mise √† jour)
  - Documentation (m√©tadonn√©es, description des colonnes)
  - Format (CSV, JSON, Excel)
  - Licence (Open Data, gratuit)
  
#### Datasets s√©lectionn√©s et justification

| Dataset | Taille | Int√©r√™t | Difficult√©s |
|---------|--------|---------|-------------|
| **Accidents v√©lo** | 80k accidents (22k IDF) | Variable cible principale | G√©olocalisation parfois impr√©cise |
| **Am√©nagements cyclables** | 143k infrastructures | Features principales d'exposition | Formats h√©t√©rog√®nes (OSM) |
| **Comptages v√©lo** | 933k mesures | Proxy du trafic cycliste | Couverture limit√©e (69 compteurs) |
| **Population INSEE** | 1287 communes IDF | Normalisation des taux | N√©cessite jointure par code INSEE |

#### Pourquoi ces choix ?
- **Compl√©mentarit√©** : accidents (sortie) + am√©nagements (features) + comptages (activit√©) + population (normalisation)
- **Granularit√© commune** : toutes les donn√©es agr√©gables au niveau communal via code INSEE
- **P√©riode coh√©rente** : donn√©es r√©centes (2015-2023)

---

### 1.2 Conception des nouvelles features (Feature Engineering)

#### 1.2.1 Variables agr√©g√©es par commune

**Pourquoi agr√©ger au niveau communal ?**
- Unit√© g√©ographique administrative stable
- Code INSEE comme cl√© de jointure unique
- √âchelle pertinente pour les politiques publiques

**Agr√©gations r√©alis√©es (fichier `01_preparation_donnees.py`)**

```python
# ACCIDENTS - Statistiques par commune
df_accidents_agg = df_accidents.groupby('code_insee').agg(
    nb_accidents=('Num_Acc', 'count'),                        # Total
    nb_accidents_graves=('grav', lambda x: (x <= 2).sum()),   # Tu√©s + hospitalis√©s
    nb_accidents_mortels=('grav', lambda x: (x == 1).sum()),  # D√©c√®s uniquement
    gravite_moyenne=('grav', 'mean'),                         # Moyenne de gravit√©
    age_moyen_victimes=('age', 'mean'),                       # Profil d√©mographique
    # Conditions environnementales
    nb_accidents_nuit=('lum', lambda x: (x.isin([2,3,4,5])).sum()),
    nb_accidents_pluie=('atm', lambda x: (x.isin([2,3,4,5,6,7])).sum()),
    nb_accidents_mouille=('surf', lambda x: (x == 2).sum())
)

# AM√âNAGEMENTS - Infrastructure cyclable
df_amenagements_agg = df_amenagements.groupby('code_insee').agg(
    nb_amenagements=('osm_id', 'count'),
    longueur_totale_amenagements=('longueur', 'sum'),         # M√®tres totaux
    longueur_moyenne_amenagement=('longueur', 'mean'),
    # Types de voies (classification OSM)
    nb_voies_principales=('highway', lambda x: x.isin(['primary','secondary','tertiary']).sum()),
    nb_voies_residentielles=('highway', lambda x: (x == 'residential').sum()),
    nb_pistes_cyclables=('highway', lambda x: (x == 'cycleway').sum()),
    # Sens de circulation
    nb_double_sens=('sens_voit', lambda x: (x == 'DOUBLE').sum()),
    nb_sens_unique=('sens_voit', lambda x: (x == 'UNIQUE').sum()),
    # Qualit√© du rev√™tement
    nb_asphalt=('revetement', lambda x: (x == 'asphalt').sum())
)
```

**Pourquoi ces variables ?**
- `nb_accidents_graves` : discrimine la gravit√©, pas juste la quantit√©
- `nb_pistes_cyclables` : infrastructure d√©di√©e vs voie partag√©e (s√©curit√© diff√©rente)
- `nb_accidents_nuit` : facteur de risque connu (visibilit√©)
- `longueur_totale_amenagements` : exposition au risque (plus d'infra = plus d'usage)

---

#### 1.2.2 Taux de risque (Feature Engineering avanc√©)

**‚û°Ô∏è TAUX 1 : Accidents par km d'am√©nagement**

```python
taux_risque_par_km = nb_accidents / (longueur_totale_amenagements / 1000)
```

**Justification :**
- Normalise par l'**exposition** √† l'infrastructure
- Une commune avec 100 km d'am√©nagements et 50 accidents est moins dangereuse qu'une commune avec 10 km et 30 accidents
- **Interpr√©tation** : "Combien d'accidents pour 1 km de piste cyclable ?"
- **Limite** : ne tient pas compte du trafic r√©el (nombre de cyclistes)

**‚û°Ô∏è TAUX 2 : Accidents pour 10 000 habitants**

```python
taux_risque_par_habitant = (nb_accidents / population) * 10000
```

**Justification :**
- Normalise par la **population** (proxy de l'activit√©)
- Permet de comparer petites et grandes communes (Paris vs village)
- **Interpr√©tation** : "Pour 10 000 habitants, combien d'accidents ?"
- **√âchelle** : 10 000 pour avoir des nombres > 1 (lisibilit√©)
- **Limite** : assume que le nombre de cyclistes est proportionnel √† la population

**‚û°Ô∏è VARIABLE BINAIRE : Risque √©lev√©**

```python
seuil_risque = df['nb_accidents'].quantile(0.75)  # 75e percentile
risque_eleve = (nb_accidents >= seuil_risque).astype(int)  # 1 ou 0
```

**Justification du seuil (75e percentile) :**
- **D√©s√©quilibre ma√Ætris√©** : 25% de communes √† risque √©lev√© (√©quilibr√© pour ML)
- √âvite les seuils arbitraires (ex: "5 accidents") qui ignorent la distribution
- **Approche data-driven** : le seuil s'adapte aux donn√©es
- Dans ce projet : seuil = 6 accidents (25% des communes ont ‚â•6 accidents)

**‚û°Ô∏è AUTRES FEATURES D√âRIV√âES**

```python
# Ratios (proportions)
ratio_pistes_cyclables = nb_pistes_cyclables / nb_amenagements
ratio_double_sens = nb_double_sens / nb_amenagements
ratio_amenagements_par_accident = nb_amenagements / nb_accidents

# Densit√©
densite_pop_amenagement = population / (longueur_totale_amenagements / 1000)

# Indicateurs binaires
est_paris = (departement == '75').astype(int)
```

**Pourquoi ces ratios ?**
- `ratio_pistes_cyclables` : qualit√© de l'infra (piste s√©par√©e = + s√©curit√©)
- `densite_pop_amenagement` : congestion potentielle
- `est_paris` : Paris a un profil tr√®s diff√©rent (forte densit√©, tourisme)

---

### 1.3 Analyse critique des r√©sultats

#### 1.3.1 M√©triques de classification

**Matrice de confusion et m√©triques d√©riv√©es**

| M√©trique | Formule | Interpr√©tation | Quand optimiser |
|----------|---------|----------------|-----------------|
| **Accuracy** | `(TP + TN) / Total` | % de pr√©dictions correctes | √âquilibr√© |
| **Precision** | `TP / (TP + FP)` | % de vrais positifs parmi les pr√©dits positifs | Co√ªt √©lev√© des faux positifs |
| **Recall** | `TP / (TP + FN)` | % de vrais positifs d√©tect√©s | Co√ªt √©lev√© des faux n√©gatifs |
| **F1-Score** | `2 √ó (Precision √ó Recall) / (Precision + Recall)` | Moyenne harmonique | Compromis P/R |
| **ROC-AUC** | Aire sous courbe ROC | Capacit√© √† discriminer | Global (seuil flexible) |

**R√©sultats du projet (Classification du risque)**

| Mod√®le | Accuracy | F1-Score | ROC-AUC | Interpr√©tation |
|--------|----------|----------|---------|----------------|
| R√©gression Logistique | 0.884 | **0.794** | **0.956** | ‚úÖ Meilleur compromis |
| Random Forest | 0.898 | 0.793 | 0.951 | ‚úÖ Accuracy l√©g√®rement sup√©rieure |
| XGBoost | 0.884 | 0.780 | 0.941 | ‚ö†Ô∏è Moins bon sur F1 |

**Analyse critique :**
- üü¢ **ROC-AUC > 0.95** : excellente capacit√© de discrimination
- üü¢ **F1 ~ 0.79** : bon √©quilibre precision/recall malgr√© le d√©s√©quilibre de classes
- üü° **Pourquoi r√©gression logistique gagne ?** 
  - Probl√®me relativement **lin√©aire** (features bien construites)
  - `class_weight='balanced'` g√®re bien le d√©s√©quilibre
  - Moins d'overfitting que les mod√®les complexes

---

#### 1.3.2 M√©triques de r√©gression

**Interpr√©tation des m√©triques**

| M√©trique | Formule | Unit√© | Interpr√©tation | Avantages | Inconv√©nients |
|----------|---------|-------|----------------|-----------|---------------|
| **RMSE** | `‚àö(Œ£(y - ≈∑)¬≤ / n)` | M√™me que y | Erreur quadratique moyenne | P√©nalise grandes erreurs | Sensible aux outliers |
| **MAE** | `Œ£|y - ≈∑| / n` | M√™me que y | Erreur absolue moyenne | Robuste aux outliers | Moins sensible aux grandes erreurs |
| **R¬≤** | `1 - SS_res/SS_tot` | Sans unit√© [0,1] | % de variance expliqu√©e | Intuitive (0-100%) | Peut √™tre n√©gatif si mod√®le tr√®s mauvais |
| **MAPE** | `100 √ó Œ£|y - ≈∑|/y / n` | % | Erreur en pourcentage | Interpr√©table en % | Undefined si y=0 |

**R√©sultats du projet (Pr√©diction nb accidents)**

| Mod√®le | RMSE | MAE | R¬≤ | Analyse |
|--------|------|-----|----|---------|
| XGBoost | 14.2 | 5.3 | **0.721** | ‚úÖ Meilleure variance expliqu√©e |
| Gradient Boosting | 14.5 | 5.5 | 0.712 | ‚úÖ Tr√®s proche |
| Random Forest | 15.1 | 5.8 | 0.688 | ‚ö†Ô∏è Un peu moins bon |
| Linear Regression | 18.3 | 7.2 | 0.534 | ‚ùå Trop simple (non-lin√©arit√©s) |

**Analyse critique :**
- üü¢ **R¬≤ ~ 0.72** : 72% de la variance expliqu√©e (correct pour donn√©es r√©elles)
- üü° **RMSE = 14 accidents** : erreur moyenne de ¬±14 accidents (√©chelle : 0-300)
- üî¥ **Limites identifi√©es** :
  - Forte asym√©trie (Paris = 300 accidents, villages = 0-5)
  - Comptages incomplets (seulement 69 compteurs pour 1124 communes)
  - Causalit√© complexe (facteurs non captur√©s : comportement, infrastructure urbaine)

---

#### 1.3.3 Analyse de distribution (Statistiques)

**Mesures d'asym√©trie et d'aplatissement**

```python
# Dans 04_analyse_taux_risque.py
skewness = stats.skew(data)      # Asym√©trie
kurtosis = stats.kurtosis(data)  # Aplatissement
cv = std / mean                  # Coefficient de variation
```

**Interpr√©tation :**
- **Skewness** :
  - `> 0` : distribution asym√©trique √† droite (queue longue vers valeurs √©lev√©es)
  - `~ 0` : distribution sym√©trique (normale)
  - `< 0` : asym√©trique √† gauche
  - Dans le projet : **skew ~ 5-8** ‚Üí fortement asym√©trique (beaucoup de petites valeurs, quelques grandes)
  
- **Kurtosis** :
  - `> 0` : distribution leptokurtique (pic pointu, queues lourdes)
  - `~ 0` : normale
  - `< 0` : platykurtique (aplatie)
  - Dans le projet : **kurt ~ 30-50** ‚Üí pics extr√™mes (Paris vs villages)

**Transformation log pour normaliser :**
```python
y_log = np.log1p(y)  # log(1 + y) pour √©viter log(0)
```
**Pourquoi ?** R√©duit l'asym√©trie, rend la distribution plus normale (meilleure performance des mod√®les lin√©aires)

---

#### 1.3.4 Corr√©lations

**Analyse de Pearson :**
```python
correlation = df['feature'].corr(df['target'])
```

**R√©sultats cl√©s (corr√©lations avec nb_accidents) :**
| Feature | Corr√©lation | Interpr√©tation |
|---------|-------------|----------------|
| `nb_amenagements` | **+0.82** | ‚úÖ Forte : plus d'infra ‚Üí plus d'accidents (causalit√© : + usage) |
| `population` | **+0.76** | ‚úÖ Forte : villes ‚Üí + accidents |
| `longueur_totale_amenagements` | **+0.79** | ‚úÖ Exposition |
| `comptage_total_commune` | **+0.65** | üü° Mod√©r√©e : donn√©es incompl√®tes |
| `ratio_pistes_cyclables` | **-0.12** | ‚ö†Ô∏è Faible n√©gative : + de pistes s√©par√©es ‚Üí - d'accidents ? |

**Analyse critique :**
- Corr√©lations fortes attendues (exposition)
- **Attention** : corr√©lation ‚â† causalit√©
  - Ex : `nb_amenagements` corr√©l√© car communes avec + d'infra ont + de cyclistes
  - Pas n√©cessairement que l'infra cause les accidents

---

### 1.4 R√©daction du rapport (LaTeX)

#### Structure d'un article scientifique

```latex
\section{Introduction}          % Contexte + objectifs
\section{Donn√©es et M√©thodologie}  % Description datasets + fusion
\section{Analyse 1 : Classification}  % Question 1 + r√©sultats
\section{Analyse 2 : R√©gression}      % Question 2 + r√©sultats
\section{Analyse 3 : Taux de risque}  % Question 3 + r√©sultats
\section{Discussion}             % Limites + interpr√©tation
\section{Conclusion}             % Synth√®se + perspectives
```

#### Bonnes pratiques LaTeX

**Tableaux :**
```latex
\begin{table}[H]
    \centering
    \caption{Performance des mod√®les}
    \label{tab:classification}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Mod√®le} & \textbf{Accuracy} & \textbf{F1} \\
        \midrule
        XGBoost & 0.884 & 0.780 \\
        \bottomrule
    \end{tabular}
\end{table}
```

**Figures :**
```latex
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{predictions_vs_reel.png}
    \caption{Pr√©dictions vs valeurs r√©elles}
    \label{fig:predictions}
\end{figure}
```

**R√©f√©rences crois√©es :**
```latex
Voir Tableau~\ref{tab:classification} et Figure~\ref{fig:predictions}
```

#### Vocabulaire technique fran√ßais

| Anglais | Fran√ßais | Exemple |
|---------|----------|---------|
| Machine Learning | Apprentissage automatique | "mod√®les d'apprentissage automatique" |
| Feature Engineering | Ing√©nierie des variables | "cr√©ation de nouvelles variables" |
| Overfitting | Surapprentissage | "risque de surapprentissage" |
| Train/Test split | S√©paration entra√Ænement/test | "ensemble d'entra√Ænement" |
| Cross-validation | Validation crois√©e | "validation crois√©e √† 5 plis" |
| Baseline | Mod√®le de r√©f√©rence | "mod√®le de base pour comparaison" |

---

---

## ü§ñ PARTIE 2 : Fusion, Pr√©paration & Mod√®les de R√©gression
*Nicolas Huyghe*

### 2.1 Fusion des datasets

#### 2.1.1 Strat√©gies de jointure (pandas)

**Types de merges et justifications :**

```python
# 1. Accidents + Am√©nagements : OUTER JOIN
df_final = pd.merge(
    df_accidents_agg,
    df_amenagements_agg,
    on='code_insee',
    how='outer',  # Garde TOUTES les communes (avec ou sans accidents)
    indicator='_merge_amenagements'
)
```
**Pourquoi `outer` ?**
- Beaucoup de communes ont des am√©nagements mais 0 accident (s√©curit√© ?)
- On veut √©tudier ces cas aussi (pr√©dire 0 accidents)
- `indicator` permet de diagnostiquer la fusion

**Statistiques de fusion du projet :**
```
Communes avec accidents ET am√©nagements : 758
Communes avec accidents uniquement      : 143 (rural, pas d'infra)
Communes avec am√©nagements uniquement   : 223 (tr√®s s√ªr ou sous-reporting ?)
```

```python
# 2. + Comptages : LEFT JOIN
df_final = pd.merge(
    df_final,
    df_comptages_communes,
    on='code_insee',
    how='left'  # Garde toutes les communes m√™me sans compteur
)
```
**Pourquoi `left` ?**
- Seulement 69 compteurs pour 1124 communes (couverture 6%)
- Ne pas perdre les communes sans compteur (mettre NaN ‚Üí fillna(0))

```python
# 3. + Population : LEFT JOIN
df_final = pd.merge(
    df_final,
    df_population,
    on='code_insee',
    how='left'
)
```

---

#### 2.1.2 Association g√©ospatiale (compteurs ‚Üí communes)

**Probl√®me :** Les compteurs n'ont pas de code INSEE, seulement des coordonn√©es GPS.

**Solution :** Distance euclidienne entre coordonn√©es

```python
from scipy.spatial.distance import cdist

# Matrices de coordonn√©es
coords_compteurs = df_compteurs[['lat', 'long']].values  # (69, 2)
coords_communes = df_communes[['lat_moyenne', 'long_moyenne']].values  # (1124, 2)

# Matrice de distances (69 x 1124)
distances = cdist(coords_compteurs, coords_communes, metric='euclidean')

# Pour chaque compteur, trouver la commune la plus proche
idx_commune_proche = distances.argmin(axis=1)  # (69,)
df_compteurs['code_insee'] = communes.iloc[idx_commune_proche]['code_insee']
```

**Limites :**
- Distance euclidienne sur lat/long ‚â† distance r√©elle (projection)
- Un compteur peut √™tre √† la fronti√®re de 2 communes
- Mieux : utiliser `geopandas` avec vraies g√©om√©tries (polygones communaux)

---

### 2.2 Pr√©paration des donn√©es (Data Cleaning)

#### 2.2.1 Gestion des valeurs manquantes

**Strat√©gies selon le type de variable :**

```python
# 1. Variables de comptage : NaN ‚Üí 0 (absence = 0)
accident_cols = ['nb_accidents', 'nb_accidents_graves']
df[accident_cols] = df[accident_cols].fillna(0).astype(int)

# 2. Ratios : NaN ‚Üí 0 (si d√©nominateur = 0)
df['taux_accidents_graves'] = df['taux_accidents_graves'].fillna(0)

# 3. Population : NaN ‚Üí garder NaN puis exclure si n√©cessaire
# (car 0 habitant n'a pas de sens, c'est vraiment une donn√©e manquante)
```

**Valeurs infinies :**
```python
# Division par 0 ‚Üí inf
df['ratio'] = df['a'] / df['b']  # Si b=0 ‚Üí inf

# Solution
df = df.replace([np.inf, -np.inf], np.nan)  # inf ‚Üí NaN
df = df.fillna(0)  # ou fillna(valeur_appropri√©e)
```

---

#### 2.2.2 Conversion de types

```python
# Codes INSEE : forcer en string avec padding
df['code_insee'] = df['com'].str.zfill(5)  # '75' ‚Üí '75000' (Paris)

# Dates
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df['annee'] = df['date'].dt.year

# Num√©riques avec gestion d'erreurs
df['lat'] = pd.to_numeric(df['lat'], errors='coerce')  # Invalides ‚Üí NaN
```

**Pourquoi `errors='coerce'` ?**
- Donn√©es r√©elles contiennent souvent des valeurs invalides ('NA', 'N/A', '')
- `coerce` transforme en NaN au lieu de lever une erreur

---

### 2.3 Impl√©mentation des mod√®les de r√©gression

#### 2.3.1 Gradient Boosting (sklearn)

**Code :**
```python
from sklearn.ensemble import GradientBoostingRegressor

model = GradientBoostingRegressor(
    n_estimators=100,      # Nombre d'arbres
    max_depth=5,           # Profondeur max de chaque arbre
    learning_rate=0.1,     # Taux d'apprentissage (par d√©faut)
    random_state=42        # Reproductibilit√©
)
model.fit(X_train, y_train)
```

**Principe du Gradient Boosting :**
1. **S√©quentiel** : chaque arbre corrige les erreurs du pr√©c√©dent
2. **Gradient descent** : optimise une fonction de perte (MSE pour r√©gression)
3. **Weak learners** : arbres peu profonds (max_depth=5)

**Pourquoi ces hyperparam√®tres ?**

| Param√®tre | Valeur | Justification |
|-----------|--------|---------------|
| `n_estimators=100` | 100 arbres | Compromis temps/performance (50 = sous-apprentissage, 500 = lent) |
| `max_depth=5` | Profondeur 5 | √âvite l'overfitting (arbres simples = r√©gularisation) |
| `learning_rate=0.1` | 0.1 (d√©faut) | Compromis stabilit√©/vitesse (0.01 = lent, 0.5 = instable) |

**Avantages :**
- ‚úÖ G√®re bien les non-lin√©arit√©s
- ‚úÖ Robuste aux outliers
- ‚úÖ Pas besoin de normalisation

**Inconv√©nients :**
- ‚ùå Sensible √† l'overfitting si `max_depth` trop grand
- ‚ùå Plus lent que Random Forest (s√©quentiel)

---

#### 2.3.2 XGBoost (Extreme Gradient Boosting)

**Code :**
```python
from xgboost import XGBRegressor

model = XGBRegressor(
    n_estimators=100,
    max_depth=5,
    learning_rate=0.1,
    random_state=42,
    verbosity=0           # Silence les logs
)
model.fit(X_train, y_train)
```

**Diff√©rences avec GB sklearn :**
| Aspect | Gradient Boosting | XGBoost |
|--------|-------------------|---------|
| **R√©gularisation** | Non | Oui (L1/L2 sur poids des feuilles) |
| **Parall√©lisation** | Non (s√©quentiel) | Oui (construction d'arbres parall√®le) |
| **Gestion NaN** | Non (erreur) | Oui (natif) |
| **Optimisation** | Standard | Cache-aware, sparsity-aware |
| **Vitesse** | R√©f√©rence | **2-10x plus rapide** |

**Pourquoi XGBoost est meilleur dans ce projet ?**
- ‚úÖ **R√©gularisation automatique** ‚Üí moins d'overfitting
- ‚úÖ **Plus rapide** : 100 arbres entra√Æn√©s en ~2s vs 10s pour GB
- ‚úÖ **G√®re mieux les features peu importantes** (pruning intelligent)
- ‚úÖ Tr√®s utilis√© en comp√©tition (Kaggle)

**Hyperparam√®tres sp√©cifiques XGBoost :**
```python
XGBRegressor(
    reg_alpha=0,       # R√©gularisation L1 (Lasso) - 0 = pas de L1
    reg_lambda=1,      # R√©gularisation L2 (Ridge) - 1 = d√©faut
    subsample=1.0,     # % d'√©chantillons par arbre (1.0 = tous)
    colsample_bytree=1.0  # % de features par arbre
)
```

---

#### 2.3.3 LightGBM (Light Gradient Boosting Machine)

**Code :**
```python
import lightgbm as lgb

model = lgb.LGBMRegressor(
    n_estimators=100,
    max_depth=5,
    random_state=42,
    verbose=-1  # Silence complet
)
```

**Diff√©rence cl√© : Leaf-wise vs Level-wise**

| Gradient Boosting / XGBoost | LightGBM |
|----------------------------|----------|
| **Level-wise** : construit l'arbre niveau par niveau | **Leaf-wise** : d√©veloppe la feuille avec plus de gain |
| Plus √©quilibr√© | Plus profond, plus rapide |
| Moins d'overfitting | Risque d'overfitting si `max_depth` trop grand |

**Quand utiliser LightGBM ?**
- ‚úÖ Tr√®s grands datasets (millions de lignes) ‚Üí + rapide que XGBoost
- ‚úÖ Features cat√©gorielles (gestion native)
- ‚ö†Ô∏è Sur petits datasets : XGBoost souvent meilleur

---

#### 2.3.4 Comparaison finale des mod√®les de r√©gression

| Mod√®le | Principe | Vitesse | Performance | Quand utiliser |
|--------|----------|---------|-------------|----------------|
| **Linear Regression** | R√©gression lin√©aire simple | ‚ö°‚ö°‚ö° Tr√®s rapide | ‚≠ê‚≠ê Faible (relations lin√©aires seulement) | Baseline, probl√®mes simples |
| **Ridge** | R√©gression lin√©aire + r√©gularisation L2 | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | Features corr√©l√©es |
| **Lasso** | R√©gression lin√©aire + r√©gularisation L1 | ‚ö°‚ö° | ‚≠ê‚≠ê | S√©lection de features (met certains coefs √† 0) |
| **Random Forest** | Ensemble d'arbres ind√©pendants (bagging) | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Bon baseline non-lin√©aire |
| **Gradient Boosting** | Boosting s√©quentiel d'arbres | ‚ö° Lent | ‚≠ê‚≠ê‚≠ê‚≠ê | R√©f√©rence sklearn |
| **XGBoost** | GB optimis√© + r√©gularisation | ‚ö°‚ö° Rapide | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **Meilleur choix g√©n√©ral** |
| **LightGBM** | GB leaf-wise | ‚ö°‚ö°‚ö° Tr√®s rapide | ‚≠ê‚≠ê‚≠ê‚≠ê | Tr√®s grands datasets |

**Dans ce projet :**
- ü•á **XGBoost** : R¬≤ = 0.721, RMSE = 14.2 ‚Üí meilleur mod√®le
- ü•à **Gradient Boosting** : R¬≤ = 0.712 ‚Üí tr√®s proche
- ü•â **Random Forest** : R¬≤ = 0.688 ‚Üí bon baseline

---

### 2.4 Pr√©paration ML (Workflow complet)

#### 2.4.1 Train/Test Split

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # 20% en test
    random_state=42,    # Reproductibilit√©
    stratify=y          # Pour classification : garde les proportions de classes
)
```

**Pourquoi 80/20 ?**
- Compromis biais-variance :
  - Plus de train ‚Üí mod√®le apprend mieux
  - Plus de test ‚Üí √©valuation fiable
- Convention : 80/20 pour ~1000 √©chantillons, 90/10 pour > 10k

**Pourquoi `random_state=42` ?**
- Reproductibilit√© : m√™me split √† chaque ex√©cution
- 42 = convention (r√©f√©rence "Hitchhiker's Guide to the Galaxy")

**Pourquoi `stratify=y` (classification) ?**
```python
# Sans stratify
y_train: [0:750, 1:250]  # Peut √™tre d√©s√©quilibr√©
y_test:  [0:150, 1:50]

# Avec stratify=y
y_train: [0:800, 1:200]  # Garde la proportion 75/25
y_test:  [0:100, 1:25]
```

---

#### 2.4.2 Normalisation (Standardization)

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit + transform
X_test_scaled = scaler.transform(X_test)        # Transform seulement
```

**Formule :**
```
X_scaled = (X - mean) / std
```

**Pourquoi normaliser ?**
- **Mod√®les sensibles √† l'√©chelle** : r√©gression logistique, SVM, r√©seaux de neurones
  - Ex : `population` (0-2M) vs `ratio_pistes_cyclables` (0-1)
  - Sans normalisation : population domine
- **Pas n√©cessaire** pour arbres (Random Forest, XGBoost) : insensibles √† l'√©chelle

**‚ö†Ô∏è IMPORTANT : fit sur train, transform sur test**
```python
# ‚ùå ERREUR (data leakage)
scaler.fit(X)  # Utilise les stats du test
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ‚úÖ CORRECT
scaler.fit(X_train)  # Stats du train uniquement
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

---

#### 2.4.3 Validation crois√©e (Cross-Validation)

```python
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(
    model, 
    X_train, 
    y_train, 
    cv=5,              # 5-fold CV
    scoring='r2'       # M√©trique
)

print(f"R¬≤ moyen: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")
```

**Principe (5-fold) :**
```
Train: [====|====|====|====|----]  Test: [----]  ‚Üí Score 1
Train: [====|====|====|----|----|  Test: [====]  ‚Üí Score 2
Train: [====|====|----|----|====]  Test: [====]  ‚Üí Score 3
Train: [====|----|----|====|====]  Test: [====]  ‚Üí Score 4
Train: [----|----|====|====|====]  Test: [====]  ‚Üí Score 5

Score final = mean(5 scores) ¬± std
```

**Pourquoi CV ?**
- √âvalue la **stabilit√©** du mod√®le
- D√©tecte l'**overfitting** : si `score_train >> score_cv`, overfitting
- Utilise **toutes les donn√©es** pour validation

**Dans le projet :**
```
XGBoost : CV R¬≤ = 0.698 (+/- 0.042)
‚Üí Mod√®le stable (faible std)
```

---

### 2.5 √âvaluation et visualisation

#### 2.5.1 Graphiques de pr√©dictions

```python
import matplotlib.pyplot as plt

plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([0, y_test.max()], [0, y_test.max()], 'r--', label='Parfait')
plt.xlabel('Valeurs r√©elles')
plt.ylabel('Pr√©dictions')
plt.title(f'XGBoost - R¬≤ = {r2:.3f}')
```

**Interpr√©tation :**
- Points sur la ligne rouge : pr√©dictions parfaites
- Points au-dessus : surestimation
- Points en-dessous : sous-estimation
- Dispersion : incertitude du mod√®le

---

#### 2.5.2 Feature Importance (arbres)

```python
importances = model.feature_importances_  # Mod√®les √† arbres uniquement
indices = np.argsort(importances)[::-1][:10]  # Top 10

plt.barh(feature_names[indices], importances[indices])
plt.xlabel('Importance')
```

**Calcul (Gini importance) :**
- Pour chaque split d'un arbre : gain = r√©duction de l'erreur
- Importance d'une feature = somme des gains pour cette feature

**Dans le projet (XGBoost) :**
```
1. longueur_totale_amenagements : 0.352  ‚Üí Tr√®s important (exposition)
2. population                    : 0.198  ‚Üí Important (activit√©)
3. comptage_total_commune        : 0.142  ‚Üí Mod√©r√© (trafic)
4. nb_amenagements               : 0.089  ‚Üí Mod√©r√©
```

---

### 2.6 Gestion du d√©s√©quilibre (Classification)

**Probl√®me :**
```
Classe 0 (risque faible)  : 843 communes (75%)
Classe 1 (risque √©lev√©)   : 281 communes (25%)
```

**Solutions impl√©ment√©es :**

#### 1. Pond√©ration des classes
```python
LogisticRegression(class_weight='balanced')
RandomForestClassifier(class_weight='balanced')
```
**Effet :** P√©nalise plus les erreurs sur la classe minoritaire

**Formule :**
```python
weight_class_0 = n_samples / (2 * n_class_0)  # 1124 / (2*843) = 0.67
weight_class_1 = n_samples / (2 * n_class_1)  # 1124 / (2*281) = 2.00
```

#### 2. Scale pos weight (XGBoost)
```python
XGBClassifier(scale_pos_weight=3)
```
**Effet :** Multiplie le poids des exemples positifs par 3
**Calcul :** `ratio = n_class_0 / n_class_1 = 843/281 = 3`

---

## üéì Concepts ML Transversaux

### Overfitting vs Underfitting

| Concept | D√©finition | Sympt√¥mes | Solutions |
|---------|------------|-----------|-----------|
| **Overfitting** | Mod√®le trop complexe, apprend le bruit | `score_train >> score_test` | ‚Üì Complexit√©, r√©gularisation, + donn√©es |
| **Underfitting** | Mod√®le trop simple | `score_train` et `score_test` faibles | ‚Üë Complexit√©, + features |
| **Good fit** | Compromis optimal | `score_train ‚âà score_test` | ‚úÖ |

**Dans le projet :**
```
XGBoost : Train R¬≤ = 0.756, Test R¬≤ = 0.721
‚Üí L√©g√®rement overfitt√© mais acceptable (√©cart < 5%)
```

---

### Boosting vs Bagging

| Aspect | Bagging (Random Forest) | Boosting (XGBoost) |
|--------|-------------------------|---------------------|
| **Principe** | Arbres ind√©pendants en parall√®le | Arbres s√©quentiels corrigeant les erreurs |
| **Biais** | Mod√©r√© | Faible |
| **Variance** | Faible (moyennage) | Mod√©r√©e |
| **Overfitting** | R√©sistant | Sensible si mal r√©gl√© |
| **Vitesse** | Rapide (parall√®le) | Lent (s√©quentiel) |
| **Performance** | Bonne | Excellente |

---

## üìä R√©sum√© des choix techniques

### Hyperparam√®tres finaux

| Mod√®le | Hyperparam√®tres | Justification |
|--------|-----------------|---------------|
| **XGBoost R√©gression** | `n_estimators=100, max_depth=5, lr=0.1` | Compromis temps/performance |
| **XGBoost Classification** | `n_estimators=100, scale_pos_weight=3` | Gestion d√©s√©quilibre |
| **Gradient Boosting** | `n_estimators=100, max_depth=5` | √âvite overfitting (arbres simples) |
| **Random Forest** | `n_estimators=100, max_depth=10` | + profond car bagging = r√©gularisation |
| **R√©gression Logistique** | `class_weight='balanced', max_iter=1000` | Convergence + √©quilibrage |

### Split et validation

| Param√®tre | Valeur | Justification |
|-----------|--------|---------------|
| `test_size` | 0.2 (20%) | Convention 80/20 pour ~1000 √©chantillons |
| `random_state` | 42 | Reproductibilit√© |
| `cv` | 5 folds | Bon compromis variance/biais |
| `stratify` | `y` (classification) | Garde proportions des classes |

---

## üé§ Conseils pour la pr√©sentation

### Parler des features
> "Nous avons cr√©√© deux **taux de risque normalis√©s** : le premier divise le nombre d'accidents par les kilom√®tres d'am√©nagements pour mesurer le risque par exposition √† l'infrastructure, le second normalise par la population pour comparer des communes de tailles diff√©rentes."

### Parler des mod√®les
> "Nous avons test√© 7 algorithmes de r√©gression. **XGBoost** s'est r√©v√©l√© le plus performant avec un **R¬≤ de 0.72**, expliquant 72% de la variance des accidents. Ce mod√®le utilise un **gradient boosting optimis√©** avec r√©gularisation L2 pour √©viter le surapprentissage."

### Parler des hyperparam√®tres
> "Nous avons fix√© `n_estimators=100` pour un compromis entre temps de calcul et performance, et `max_depth=5` pour limiter la complexit√© de chaque arbre et √©viter l'overfitting sur notre dataset de 1124 communes."

### Parler des limites
> "Notre analyse pr√©sente certaines limites : les donn√©es de comptage couvrent seulement 6% des communes, et nous n'avons pas captur√© certains facteurs comportementaux. N√©anmoins, un **R¬≤ de 0.72** reste satisfaisant pour des donn√©es r√©elles avec une forte h√©t√©rog√©n√©it√©."

---

## üìö R√©f√©rences et ressources

### Librairies Python
- **pandas** : manipulation de donn√©es tabulaires
- **numpy** : calcul num√©rique
- **scikit-learn** : mod√®les ML, preprocessing, m√©triques
- **xgboost** : gradient boosting optimis√©
- **lightgbm** : gradient boosting rapide
- **matplotlib/seaborn** : visualisation
- **scipy** : statistiques, distances

### Concepts cl√©s √† ma√Ætriser
1. ‚úÖ Feature engineering (agr√©gation, ratios, normalisation)
2. ‚úÖ Fusion de datasets (merges, jointures spatiales)
3. ‚úÖ Pr√©processing (gestion NaN, outliers, types)
4. ‚úÖ Train/test split + validation crois√©e
5. ‚úÖ M√©triques (R¬≤, RMSE, F1, ROC-AUC)
6. ‚úÖ Mod√®les de r√©gression (lin√©aires, arbres, boosting)
7. ‚úÖ Hyperparam√®tres (justification des choix)
8. ‚úÖ Analyse critique (limites, biais, causalit√©)

---

**Bonne pr√©sentation ! üöÄ**
