#!/usr/bin/env python3
"""
Script d'analyse ML pour la pr√©diction du nombre d'accidents de v√©lo par commune.

Question: Pr√©dire le nombre d'accidents dans une commune en fonction du nombre d'am√©nagements.

Ce script compare plusieurs algorithmes de r√©gression:
- R√©gression Lin√©aire
- Ridge Regression
- Lasso Regression
- Random Forest Regressor
- Gradient Boosting Regressor
- XGBoost Regressor
- LightGBM Regressor
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,
                              mean_absolute_percentage_error)
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

warnings.filterwarnings('ignore')

# Configuration
DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data')
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), '..', 'outputs')
DATASET_FILE = os.path.join(DATA_DIR, 'dataset_final_idf.csv')

# Cr√©er le dossier outputs s'il n'existe pas
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Configuration des graphiques
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10

def charger_donnees():
    """Charge et pr√©pare les donn√©es pour la r√©gression."""
    print("=" * 70)
    print("CHARGEMENT DES DONN√âES")
    print("=" * 70)
    
    df = pd.read_csv(DATASET_FILE)
    print(f"Dataset charg√©: {len(df)} communes")
    
    # Features pour la r√©gression
    feature_cols = [
        'nb_amenagements',
        'longueur_totale_amenagements',
        'longueur_moyenne_amenagement',
        'nb_voies_principales',
        'nb_voies_residentielles',
        'nb_pistes_cyclables',
        'nb_double_sens',
        'nb_sens_unique',
        'nb_asphalt',
        'nb_autres_revetements',
        'ratio_pistes_cyclables',
        'ratio_double_sens',
        'nb_compteurs',
        'comptage_total_commune',
        'comptage_moyen_commune',
        'est_paris'
    ]
    
    # V√©rifier que les colonnes existent
    available_cols = [col for col in feature_cols if col in df.columns]
    print(f"Features disponibles: {len(available_cols)}/{len(feature_cols)}")
    
    # Pr√©parer X et y
    X = df[available_cols].copy()
    y = df['nb_accidents'].copy()
    
    # Remplacer les valeurs infinies et NaN
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(0)
    
    print(f"\nStatistiques de la variable cible (nb_accidents):")
    print(y.describe())
    
    return X, y, available_cols, df

def preparer_donnees(X, y):
    """Pr√©pare les donn√©es pour l'entra√Ænement."""
    print("\n" + "=" * 70)
    print("PR√âPARATION DES DONN√âES")
    print("=" * 70)
    
    # Split train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    print(f"Ensemble d'entra√Ænement: {len(X_train)} communes")
    print(f"Ensemble de test: {len(X_test)} communes")
    
    # Normalisation
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler

def definir_modeles():
    """D√©finit les mod√®les de r√©gression √† comparer."""
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge Regression': Ridge(alpha=1.0, random_state=42),
        'Lasso Regression': Lasso(alpha=0.1, random_state=42, max_iter=10000),
        'Random Forest': RandomForestRegressor(
            n_estimators=100, random_state=42, n_jobs=-1, max_depth=10
        ),
        'Gradient Boosting': GradientBoostingRegressor(
            n_estimators=100, random_state=42, max_depth=5
        ),
        'XGBoost': XGBRegressor(
            n_estimators=100, random_state=42, max_depth=5,
            learning_rate=0.1
        ),
        'LightGBM': LGBMRegressor(
            n_estimators=100, random_state=42, max_depth=5,
            verbose=-1
        )
    }
    return models

def evaluer_modeles(models, X_train_scaled, X_test_scaled, y_train, y_test):
    """√âvalue tous les mod√®les et retourne les r√©sultats."""
    print("\n" + "=" * 70)
    print("√âVALUATION DES MOD√àLES")
    print("=" * 70)
    
    results = []
    trained_models = {}
    predictions = {}
    
    for name, model in models.items():
        print(f"\n{'‚îÄ' * 50}")
        print(f"Entra√Ænement: {name}")
        print(f"{'‚îÄ' * 50}")
        
        # Entra√Ænement
        model.fit(X_train_scaled, y_train)
        trained_models[name] = model
        
        # Pr√©dictions
        y_pred = model.predict(X_test_scaled)
        # S'assurer que les pr√©dictions sont positives
        y_pred = np.maximum(y_pred, 0)
        predictions[name] = y_pred
        
        # M√©triques
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        # MAPE (attention aux valeurs nulles)
        mask = y_test > 0
        if mask.sum() > 0:
            mape = mean_absolute_percentage_error(y_test[mask], y_pred[mask]) * 100
        else:
            mape = np.nan
        
        # Validation crois√©e (R¬≤)
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, 
                                     scoring='r2')
        
        results.append({
            'Mod√®le': name,
            'RMSE': rmse,
            'MAE': mae,
            'R¬≤': r2,
            'MAPE (%)': mape,
            'CV R¬≤ (mean)': cv_scores.mean(),
            'CV R¬≤ (std)': cv_scores.std()
        })
        
        print(f"  RMSE:     {rmse:.2f}")
        print(f"  MAE:      {mae:.2f}")
        print(f"  R¬≤:       {r2:.3f}")
        print(f"  MAPE:     {mape:.1f}%")
        print(f"  CV R¬≤:    {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")
    
    return pd.DataFrame(results), trained_models, predictions

def afficher_tableau_comparatif(results_df):
    """Affiche un tableau comparatif des r√©sultats."""
    print("\n" + "=" * 70)
    print("TABLEAU COMPARATIF DES MOD√àLES")
    print("=" * 70)
    
    # Trier par R¬≤
    results_sorted = results_df.sort_values('R¬≤', ascending=False)
    
    print("\n" + results_sorted.to_string(index=False))
    
    # Sauvegarder
    results_sorted.to_csv(os.path.join(OUTPUT_DIR, 'regression_results.csv'), index=False)
    
    return results_sorted

def tracer_predictions_vs_reel(predictions, y_test, best_model_name):
    """Trace les pr√©dictions vs valeurs r√©elles."""
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    # S√©lectionner 4 mod√®les
    models_to_plot = ['Linear Regression', 'Random Forest', 'XGBoost', best_model_name]
    models_to_plot = [m for m in models_to_plot if m in predictions][:4]
    
    for ax, model_name in zip(axes.flatten(), models_to_plot):
        y_pred = predictions[model_name]
        
        ax.scatter(y_test, y_pred, alpha=0.5, edgecolors='k', linewidth=0.5)
        
        # Ligne parfaite
        max_val = max(y_test.max(), y_pred.max())
        ax.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Pr√©diction parfaite')
        
        # R¬≤ sur le graphique
        r2 = r2_score(y_test, y_pred)
        ax.text(0.05, 0.95, f'R¬≤ = {r2:.3f}', transform=ax.transAxes, 
                fontsize=12, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        ax.set_xlabel('Valeurs r√©elles', fontsize=10)
        ax.set_ylabel('Pr√©dictions', fontsize=10)
        ax.set_title(model_name, fontsize=12)
        ax.legend(loc='lower right')
    
    plt.suptitle('Pr√©dictions vs Valeurs R√©elles - Nombre d\'Accidents', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'predictions_vs_reel.png'), dpi=150)
    plt.close()
    print(f"\n‚úì Graphique pr√©dictions vs r√©el sauvegard√©: {OUTPUT_DIR}/predictions_vs_reel.png")

def tracer_predictions_vs_reel_zoom(predictions, y_test, best_model_name):
    """Trace les pr√©dictions vs valeurs r√©elles - version zoom√©e sur les petites valeurs."""
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    # S√©lectionner 4 mod√®les
    models_to_plot = ['Linear Regression', 'Random Forest', 'XGBoost', best_model_name]
    models_to_plot = [m for m in models_to_plot if m in predictions][:4]
    
    # D√©finir la limite de zoom (ex: 75e percentile des valeurs)
    zoom_limit = np.percentile(y_test, 90)
    zoom_limit = max(zoom_limit, 50)  # Au moins 50 accidents
    
    for ax, model_name in zip(axes.flatten(), models_to_plot):
        y_pred = predictions[model_name]
        
        # Filtrer pour le zoom
        mask = (y_test <= zoom_limit) & (y_pred <= zoom_limit)
        y_test_zoom = y_test[mask]
        y_pred_zoom = y_pred[mask]
        
        ax.scatter(y_test_zoom, y_pred_zoom, alpha=0.5, edgecolors='k', linewidth=0.5)
        
        # Ligne parfaite
        ax.plot([0, zoom_limit], [0, zoom_limit], 'r--', linewidth=2, label='Pr√©diction parfaite')
        
        # R¬≤ sur les donn√©es zoom√©es
        if len(y_test_zoom) > 1:
            r2 = r2_score(y_test_zoom, y_pred_zoom)
            ax.text(0.05, 0.95, f'R¬≤ = {r2:.3f}\n(n={len(y_test_zoom)})', transform=ax.transAxes, 
                    fontsize=12, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        ax.set_xlabel('Valeurs r√©elles', fontsize=10)
        ax.set_ylabel('Pr√©dictions', fontsize=10)
        ax.set_title(model_name, fontsize=12)
        ax.set_xlim(0, zoom_limit)
        ax.set_ylim(0, zoom_limit)
        ax.legend(loc='lower right')
    
    plt.suptitle(f'Pr√©dictions vs Valeurs R√©elles - Zoom (‚â§ {int(zoom_limit)} accidents)', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'predictions_vs_reel_zoom.png'), dpi=150)
    plt.close()
    print(f"‚úì Graphique pr√©dictions vs r√©el (zoom) sauvegard√©: {OUTPUT_DIR}/predictions_vs_reel_zoom.png")

def tracer_residus(predictions, y_test, best_model_name):
    """Trace l'analyse des r√©sidus pour le meilleur mod√®le."""
    y_pred = predictions[best_model_name]
    residus = y_test.values - y_pred
    
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))
    
    # Distribution des r√©sidus
    axes[0].hist(residus, bins=50, edgecolor='black', alpha=0.7)
    axes[0].axvline(x=0, color='r', linestyle='--', linewidth=2)
    axes[0].set_xlabel('R√©sidu', fontsize=12)
    axes[0].set_ylabel('Fr√©quence', fontsize=12)
    axes[0].set_title('Distribution des R√©sidus', fontsize=12)
    
    # R√©sidus vs Pr√©dictions
    axes[1].scatter(y_pred, residus, alpha=0.5, edgecolors='k', linewidth=0.5)
    axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)
    axes[1].set_xlabel('Pr√©dictions', fontsize=12)
    axes[1].set_ylabel('R√©sidus', fontsize=12)
    axes[1].set_title('R√©sidus vs Pr√©dictions', fontsize=12)
    
    # Q-Q plot
    from scipy import stats
    stats.probplot(residus, dist="norm", plot=axes[2])
    axes[2].set_title('Q-Q Plot', fontsize=12)
    
    plt.suptitle(f'Analyse des R√©sidus - {best_model_name}', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'residuals_analysis.png'), dpi=150)
    plt.close()
    print(f"‚úì Analyse des r√©sidus sauvegard√©e: {OUTPUT_DIR}/residuals_analysis.png")

def tracer_importance_features(trained_models, feature_names):
    """Trace l'importance des features pour les mod√®les bas√©s sur les arbres."""
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    
    tree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']
    
    for ax, model_name in zip(axes.flatten(), tree_models):
        if model_name in trained_models:
            model = trained_models[model_name]
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1][:10]  # Top 10
            
            colors = plt.cm.plasma(np.linspace(0.2, 0.8, len(indices)))
            ax.barh(range(len(indices)), importances[indices], color=colors)
            ax.set_yticks(range(len(indices)))
            ax.set_yticklabels([feature_names[i] for i in indices])
            ax.set_xlabel('Importance')
            ax.set_title(f'{model_name}')
            ax.invert_yaxis()
    
    plt.suptitle("Importance des Features par Mod√®le (R√©gression)", fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance_regression.png'), dpi=150)
    plt.close()
    print(f"‚úì Importance des features sauvegard√©e: {OUTPUT_DIR}/feature_importance_regression.png")

def tracer_comparaison_metriques(results_df):
    """Trace un graphique de comparaison des m√©triques."""
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # R¬≤ par mod√®le
    results_sorted = results_df.sort_values('R¬≤', ascending=True)
    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(results_sorted)))
    
    axes[0].barh(results_sorted['Mod√®le'], results_sorted['R¬≤'], color=colors)
    axes[0].set_xlabel('R¬≤ Score', fontsize=12)
    axes[0].set_title('Coefficient de D√©termination (R¬≤)', fontsize=12)
    axes[0].axvline(x=0, color='gray', linestyle='--', alpha=0.5)
    
    # RMSE par mod√®le
    results_sorted_rmse = results_df.sort_values('RMSE', ascending=False)
    colors_rmse = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(results_sorted_rmse)))
    
    axes[1].barh(results_sorted_rmse['Mod√®le'], results_sorted_rmse['RMSE'], color=colors_rmse)
    axes[1].set_xlabel('RMSE', fontsize=12)
    axes[1].set_title('Erreur Quadratique Moyenne (RMSE)', fontsize=12)
    
    plt.suptitle('Comparaison des Performances des Mod√®les', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'metrics_comparison_regression.png'), dpi=150)
    plt.close()
    print(f"‚úì Comparaison des m√©triques sauvegard√©e: {OUTPUT_DIR}/metrics_comparison_regression.png")

def tracer_correlation_features(X, y, feature_names):
    """Trace la corr√©lation entre les features et la variable cible."""
    # Calculer les corr√©lations
    correlations = []
    for col in feature_names:
        corr = X[col].corr(y)
        correlations.append({'Feature': col, 'Corr√©lation': corr})
    
    corr_df = pd.DataFrame(correlations).sort_values('Corr√©lation', ascending=True)
    
    plt.figure(figsize=(10, 8))
    colors = plt.cm.RdYlGn(np.linspace(0, 1, len(corr_df)))
    colors = [plt.cm.RdYlGn(0.5 + c/2) for c in corr_df['Corr√©lation']]
    
    plt.barh(corr_df['Feature'], corr_df['Corr√©lation'], color=colors)
    plt.xlabel('Corr√©lation avec nb_accidents', fontsize=12)
    plt.title('Corr√©lation des Features avec le Nombre d\'Accidents', fontsize=14)
    plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, 'correlation_features.png'), dpi=150)
    plt.close()
    print(f"‚úì Corr√©lation des features sauvegard√©e: {OUTPUT_DIR}/correlation_features.png")

def generer_conclusion(results_df, best_model_name, X, y):
    """G√©n√®re une conclusion textuelle de l'analyse."""
    print("\n" + "=" * 70)
    print("CONCLUSION")
    print("=" * 70)
    
    best_row = results_df[results_df['Mod√®le'] == best_model_name].iloc[0]
    
    # Trouver la feature la plus corr√©l√©e
    correlations = {col: X[col].corr(y) for col in X.columns}
    top_feature = max(correlations, key=lambda k: abs(correlations[k]))
    top_corr = correlations[top_feature]
    
    conclusion = f"""
ANALYSE DE R√âGRESSION - PR√âDICTION DU NOMBRE D'ACCIDENTS DE V√âLO
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

OBJECTIF:
Pr√©dire le nombre d'accidents de v√©lo par commune en √éle-de-France
en fonction des am√©nagements cyclables disponibles.

MEILLEUR MOD√àLE: {best_model_name}

PERFORMANCES:
  ‚Ä¢ RMSE:     {best_row['RMSE']:.2f} accidents
  ‚Ä¢ MAE:      {best_row['MAE']:.2f} accidents
  ‚Ä¢ R¬≤:       {best_row['R¬≤']:.3f}
  ‚Ä¢ CV R¬≤:    {best_row['CV R¬≤ (mean)']:.3f} (+/- {best_row['CV R¬≤ (std)']:.3f})

FEATURE LA PLUS IMPORTANTE:
  ‚Ä¢ {top_feature} (corr√©lation = {top_corr:.3f})

INTERPR√âTATION:
- Le mod√®le explique {best_row['R¬≤']*100:.1f}% de la variance du nombre d'accidents.
- L'erreur moyenne de pr√©diction est de {best_row['MAE']:.1f} accidents par commune.
- Les communes avec plus d'am√©nagements cyclables ont g√©n√©ralement 
  plus d'accidents enregistr√©s (effet de trafic cycliste plus important).

INSIGHTS CL√âS:
1. Le nombre d'am√©nagements cyclables est fortement corr√©l√© au nombre 
   d'accidents, mais cela refl√®te aussi une utilisation plus intensive.
2. Les communes parisiennes ont un profil sp√©cifique avec beaucoup 
   d'am√©nagements ET beaucoup d'accidents.
3. Pour r√©duire les accidents, il faut am√©liorer la qualit√© des 
   am√©nagements, pas seulement leur quantit√©.

RECOMMANDATIONS:
1. Privil√©gier les pistes cyclables s√©par√©es de la circulation.
2. Am√©liorer l'√©clairage sur les voies cyclables.
3. Cibler les communes √† haut risque avec peu d'am√©nagements.
"""
    
    print(conclusion)
    
    # Sauvegarder la conclusion
    with open(os.path.join(OUTPUT_DIR, 'conclusion_regression.txt'), 'w') as f:
        f.write(conclusion)
    print(f"\n‚úì Conclusion sauvegard√©e: {OUTPUT_DIR}/conclusion_regression.txt")

def main():
    """Fonction principale."""
    print("\n" + "‚ñà" * 70)
    print("  PR√âDICTION DU NOMBRE D'ACCIDENTS DE V√âLO - √éLE-DE-FRANCE")
    print("‚ñà" * 70)
    
    # 1. Charger les donn√©es
    X, y, feature_names, df = charger_donnees()
    
    # 2. Pr√©parer les donn√©es
    X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler = \
        preparer_donnees(X, y)
    
    # 3. D√©finir les mod√®les
    models = definir_modeles()
    
    # 4. √âvaluer les mod√®les
    results_df, trained_models, predictions = evaluer_modeles(
        models, X_train_scaled, X_test_scaled, y_train, y_test
    )
    
    # 5. Afficher le tableau comparatif
    results_sorted = afficher_tableau_comparatif(results_df)
    
    # 6. Identifier le meilleur mod√®le (par R¬≤)
    best_model_name = results_sorted.iloc[0]['Mod√®le']
    print(f"\nüèÜ Meilleur mod√®le: {best_model_name}")
    
    # 7. Visualisations
    print("\n" + "=" * 70)
    print("G√âN√âRATION DES VISUALISATIONS")
    print("=" * 70)
    
    tracer_predictions_vs_reel(predictions, y_test, best_model_name)
    tracer_predictions_vs_reel_zoom(predictions, y_test, best_model_name)
    tracer_residus(predictions, y_test, best_model_name)
    tracer_importance_features(trained_models, feature_names)
    tracer_comparaison_metriques(results_sorted)
    tracer_correlation_features(X, y, feature_names)
    
    # 8. Conclusion
    generer_conclusion(results_sorted, best_model_name, X, y)
    
    print("\n" + "‚ñà" * 70)
    print("  ‚úÖ ANALYSE DE R√âGRESSION TERMIN√âE")
    print("‚ñà" * 70)
    
    return results_sorted, trained_models

if __name__ == "__main__":
    results, models = main()
